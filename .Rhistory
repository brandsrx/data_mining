stringsAsFactors = FALSE
)
# Tokenizar y limpiar
data("stop_words")
tokens <- df %>%
unnest_tokens(palabra, texto) %>%
anti_join(stop_words, by = c("palabra" = "word"))
tfidf <- tokens %>%
count(documento, palabra, sort = TRUE) %>%
bind_tf_idf(palabra, documento, n) %>%
arrange(desc(tf_idf))
# Mostrar top 10 palabras más relevantes
print(tfidf %>% slice_max(tf_idf, n = 10))
tfidf %>%
slice_max(tf_idf, n = 10) %>%
ggplot(aes(x = reorder(palabra, tf_idf), y = tf_idf)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Palabras más relevantes (TF-IDF)",
x = "Palabra", y = "TF-IDF") +
theme_minimal()
url <- "https://books.toscrape.com/"
pagina <- read_html(url)
citas <- pagina %>%
html_nodes(".text") %>%
html_text()
# Crear dataframe con las citas
df <- data.frame(
documento = 1:length(citas),
texto = citas,
stringsAsFactors = FALSE
)
# Tokenizar y limpiar
data("stop_words")
tokens <- df %>%
unnest_tokens(palabra, texto) %>%
anti_join(stop_words, by = c("palabra" = "word"))
# Calcular TF-IDF
tfidf <- tokens %>%
count(documento, palabra, sort = TRUE) %>%
bind_tf_idf(palabra, documento, n) %>%
arrange(desc(tf_idf))
# Mostrar top 10 palabras más relevantes
print(tfidf %>% slice_max(tf_idf, n = 10))
# Graficar
tfidf %>%
slice_max(tf_idf, n = 10) %>%
ggplot(aes(x = reorder(palabra, tf_idf), y = tf_idf)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Palabras más relevantes (TF-IDF)",
x = "Palabra", y = "TF-IDF") +
theme_minimal()
# extraer datos de una página web
url <- "https://books.toscrape.com/"
pagina <- read_html(url)
#identificar y extraer la tabla de interes en este caso la 1
tablas <- html_table(pagina)
print(tablas)
tabla_interes <- tablas[[1]] # Selecciona la primera tabla
print(tablas)
# se puede extraer titulos de la pagina
url <- "https://books.toscrape.com/"
pagina <- read_html(url)
titulos <- pagina %>%
html_elements("p") %>%
html_text()
print(titulos)
library(rvest)
# extraer datos de una página web
url <- "https://books.toscrape.com/"
pagina <- read_html(url)
print(pagin)
print(pagina)
#identificar y extraer la tabla de interes en este caso la 1
tablas <- html_table(pagina)
print(tablas)
# 1. Extraer datos de la página
url <- "https://books.toscrape.com/"
pagina <- read_html(url)
titulos <- pagina %>%
html_elements(".product_pod h3 a") %>%
html_attr("title")
precios <- pagina %>%
html_elements(".price_color") %>%
html_text()
tabla_interes <- data.frame(
titulo = titulos,
precio = precios,
stringsAsFactors = FALSE
)
# Mostrar los primeros registros
print(head(tabla_interes))
data("stop_words")
tfidf <- tabla_interes %>%
mutate(documento = row_number()) %>%
unnest_tokens(palabra, titulo) %>%
anti_join(stop_words, by = c("palabra" = "word")) %>%
count(documento, palabra, sort = TRUE) %>%
bind_tf_idf(palabra, documento, n) %>%
arrange(desc(tf_idf))
# Mostrar las 10 palabras más relevantes
print(tfidf %>% slice_max(tf_idf, n = 10))
tfidf %>%
slice_max(tf_idf, n = 10) %>%
ggplot(aes(x = reorder(palabra, tf_idf), y = tf_idf)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Palabras más relevantes (TF-IDF)",
x = "Palabra", y = "TF-IDF") +
theme_minimal()
# Mostrar los primeros registros
print(head(tabla_interes))
# 2. Revisar la URL de Wikipedia
url <- "https://nodejs.org/docs/latest/api/documentation.html"
pagina <- read_html(url)
enlaces <- pagina %>%
html_nodes("a") %>% # Selecciona todos los nodos 'a' (enlaces)
html_attr("href") # Extrae el atributo 'href' (la URL del enlace)
print(enlaces)
# 4. Para extraer tablas
tablas <- html_table(pagina) # Extrae todas las tablas como una lista de dataframes
print(tablas)
tabla_interes <- tablas[[1]]
print(tabla_interes)
enlaces_df <- data.frame(enlaces = enlaces)
enlaces_df %>%
head() %>%
print()
#Ejemplo de web scarping
# activar de web scarpung
install.packages("rvest")
# Ejemplo de web crawling
library(rvest)
library(dplyr)
# 2. Revisar la URL de Wikipedia
url <- "https://nodejs.org/docs/latest/api/documentation.html"
pagina <- read_html(url)
install.packages("rvest")
url <- "https://nodejs.org/docs/latest/api/documentation.html"
pagina <- read_html(url)
enlaces <- pagina %>%
html_nodes("a") %>% # Selecciona todos los nodos 'a' (enlaces)
html_attr("href") # Extrae el atributo 'href' (la URL del enlace)
enlaces <- pagina %>%
html_nodes("a") %>% # Selecciona todos los nodos 'a' (enlaces)
html_attr("href") # Extrae el atributo 'href' (la URL del enlace)
print(enlaces)
# Mostrar los primeros registros
print(head(tabla_interes))
setwd("~/Documents/data_mining")
# --- Visualización: Top 10 propiedades más caras ---
all_properties %>%
arrange(desc(Precio)) %>%
head(10) %>%
ggplot(aes(x = reorder(Titulo, Precio), y = Precio)) +
geom_col(fill = "tomato") +
coord_flip() +
labs(title = "Top 10 propiedades más caras en La Paz",
x = "Propiedad", y = "Precio (Bs.)") +
theme_minimal()
# --- Configuración ---
node_path <- "/home/brandon/.nvm/versions/node/v22.19.0/bin/node"
scrape_script <- "scrape.js"
url <- "https://pedidos.polloscopacabana.com/"
get_html <- function(url) {
res <- run(node_path, c(scrape_script, url), echo = FALSE)
res$stdout
}
html <- get_html(url)
library(processx)
library(rvest)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)
node_path <- "/home/brandon/.nvm/versions/node/v22.19.0/bin/node"
scrape_script <- "scrape.js"
url <- "https://pedidos.polloscopacabana.com/"
get_html <- function(url) {
res <- run(node_path, c(scrape_script, url), echo = FALSE)
res$stdout
}
html <- get_html(url)
page <- read_html(html)
# --- Extraer menú de Pollo ---
listas_prod <- page %>% html_nodes("ul.li_prod li")
# Ver cuántas se encontraron
length(listas_prod)
# Extraer el HTML completo de cada lista
listas_html <- lapply(listas_prod, function(ul) {
html_text(ul, trim = TRUE)
})
menu_completo <- data.frame(Nombre = character(),
Precio = numeric(),
stringsAsFactors = FALSE)
for (s in listas_html) {
s_clean <- str_squish(s)  # " + Copalitos Solo Pollo Bs 39"
# Quitar el símbolo + al inicio
s_clean <- str_remove(s_clean, "^\\+")  # "Copalitos Solo Pollo Bs 39"
# Extraer todo antes de "Bs" como nombre
nombre <- str_extract(s_clean, ".*(?=Bs)")
nombre <- str_trim(nombre)
precio <- str_extract(s_clean, "Bs\\s*[0-9]+[.,]?[0-9]*")
precio_num <- precio %>%
str_replace("Bs\\s*", "") %>%  # quitar "Bs "
str_replace(",", ".") %>%      # convertir coma decimal a punto
as.numeric()
menu_completo <- rbind(menu_completo,
data.frame(Nombre = nombre,
Precio = precio_num,
stringsAsFactors = FALSE))
}
library(ggplot2)
library(dplyr)
library(stringr)
# Supongamos que tu data.frame se llama menu_completo
menu_completo <- menu_completo %>%
mutate(Nombre = str_wrap(Nombre, width = 15))  # Ajustar texto largo para que se vea bien
# Gráfico de barras horizontal
ggplot(menu_completo, aes(x = reorder(Nombre, Precio), y = Precio, fill = Precio)) +
geom_col(show.legend = FALSE) +
coord_flip() +  # barras horizontales
geom_text(aes(label = Precio), hjust = -0.1, size = 3) +  # mostrar precio al final de la barra
scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
labs(title = "Precios de Menús Pollos Copacabana",
x = "Producto",
y = "Precio (Bs.)") +
theme_minimal(base_size = 12)
print(menu_completo)
# Datos del menú
menu_data <- menu_completo
# Crear categorías
menu_data$Categoria <- ifelse(grepl("Combo", menu_data$Nombre), "Combos",
ifelse(grepl("Balde", menu_data$Nombre), "Baldes Familiares",
ifelse(grepl("Hamburguesa|Wrap", menu_data$Nombre), "Hamburguesas y Wraps",
ifelse(grepl("Papa|Arroz|Coleslaw|Ensalada", menu_data$Nombre), "Acompañamientos",
ifelse(grepl("Gaseosa", menu_data$Nombre), "Bebidas",
ifelse(grepl("Pie|Mousse|Donut", menu_data$Nombre), "Postres",
"Platos Principales"))))))
# Tipo de producto
menu_data$Tipo <- ifelse(grepl("Combo", menu_data$Nombre), "Combo",
ifelse(grepl("con Papa|con Arroz", menu_data$Nombre), "Con Acompañamiento",
ifelse(grepl("Solo", menu_data$Nombre), "Solo",
"Individual")))
# Tamaño
menu_data$Tamano <- ifelse(grepl("Personal", menu_data$Nombre), "Personal",
ifelse(grepl("Mediano|Mediana", menu_data$Nombre), "Mediano",
ifelse(grepl("Grande|XL|Doble|12", menu_data$Nombre), "Grande",
"Regular")))
# Segmento de precio
menu_data$Segmento <- cut(menu_data$Precio,
breaks = c(0, 15, 35, 60, 200),
labels = c("Económico", "Medio", "Premium", "Familiar"))
# Nombre corto para visualizaciones
menu_data$Nombre_Corto <- gsub(" con Papa o Arroz| con Papa| con Arroz", "", menu_data$Nombre)
menu_data$Nombre_Corto <- gsub("Combo ", "", menu_data$Nombre_Corto)
# Configurar ventana para múltiples gráficos
par(mfrow = c(1, 1))
# --- GRÁFICO 1: Distribución de Precios (Histograma) ---
hist(menu_data$Precio,
breaks = 20,
col = "#FF6B6B",
border = "white",
main = "Distribución de Precios en el Menú",
xlab = "Precio (Bs.)",
ylab = "Frecuencia",
las = 1)
# --- GRÁFICO 2: Top 10 Productos Más Caros ---
top10 <- menu_data[order(menu_data$Precio, decreasing = TRUE)[1:10], ]
par(mar = c(5, 12, 4, 2))
barplot(top10$Precio,
names.arg = top10$Nombre_Corto,
horiz = TRUE,
col = colorRampPalette(c("#FF6B6B", "#4ECDC4"))(10),
border = "white",
main = "Top 10: Productos Más Caros",
xlab = "Precio (Bs.)",
las = 1,
cex.names = 0.8)
text(top10$Precio - 5, 1:10, labels = paste("Bs.", top10$Precio),
col = "white", font = 2, cex = 0.9)
# --- GRÁFICO 3: Precios por Categoría (Boxplot) ---
par(mar = c(8, 5, 4, 2))
boxplot(Precio ~ Categoria, data = menu_data,
col = brewer.pal(7, "Set2"),
border = "black",
main = "Distribución de Precios por Categoría",
ylab = "Precio (Bs.)",
xlab = "",
las = 2,
notch = TRUE,
outline = TRUE)
# --- GRÁFICO 4: Cantidad de Productos por Categoría ---
par(mar = c(5, 4, 4, 2))
cat_counts <- table(menu_data$Categoria)
colores_cat <- brewer.pal(length(cat_counts), "Paired")
barplot(cat_counts,
col = colores_cat,
border = "black",
main = "Cantidad de Productos por Categoría",
ylab = "Cantidad de Productos",
las = 2,
cex.names = 0.8)
# --- GRÁFICO 4: Cantidad de Productos por Categoría ---
par(mar = c(8, 5, 4, 2))
cat_counts <- table(menu_data$Categoria)
n_categorias <- length(cat_counts)
colores_cat <- brewer.pal(max(3, min(n_categorias, 12)), "Paired")[1:n_categorias]
# --- GRÁFICO 5: Segmentación de Precios (Pie Chart) ---
segmento_counts <- table(menu_data$Segmento)
porcentajes <- round(100 * segmento_counts / sum(segmento_counts), 1)
pie(segmento_counts,
labels = paste(names(segmento_counts), "\n",
segmento_counts, " items\n(", porcentajes, "%)", sep = ""),
col = c("#66C2A5", "#FC8D62", "#8DA0CB", "#E78AC3"),
main = "Segmentación del Menú por Precio",
border = "white",
cex = 0.9)
# --- GRÁFICO 9: Gráfico de Barras Apiladas con ggplot2 ---
p9 <- ggplot(menu_data, aes(x = Categoria, fill = Segmento)) +
geom_bar(color = "black", alpha = 0.8) +
geom_text(stat = 'count', aes(label = after_stat(count)),
position = position_stack(vjust = 0.5),
color = "white", fontface = "bold") +
scale_fill_manual(values = c("#66C2A5", "#FC8D62", "#8DA0CB", "#E78AC3")) +
labs(title = "Composición del Menú: Categoría vs Segmento de Precio",
x = "Categoría",
y = "Cantidad de Productos",
fill = "Segmento") +
theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", hjust = 0.5, size = 14),
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "right"
)
print(p9)
# --- GRÁFICO 10: Comparación de Tamaños ---
p10 <- ggplot(menu_data, aes(x = Tamano, y = Precio, fill = Tamano)) +
geom_boxplot(alpha = 0.7, outlier.colour = "red", outlier.size = 3) +
geom_jitter(width = 0.2, alpha = 0.3, size = 2) +
scale_fill_brewer(palette = "Set3") +
labs(title = "Distribución de Precios por Tamaño",
x = "Tamaño",
y = "Precio (Bs.)") +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", hjust = 0.5),
legend.position = "none"
)
print(p10)
# --- GRÁFICO 11: Interactivo con Plotly ---
p11 <- plot_ly(menu_data,
x = ~Categoria,
y = ~Precio,
color = ~Segmento,
type = "box",
colors = c("#66C2A5", "#FC8D62", "#8DA0CB", "#E78AC3")) %>%
layout(
title = "Análisis Interactivo: Precio por Categoría y Segmento",
xaxis = list(title = "Categoría"),
yaxis = list(title = "Precio (Bs.)"),
boxmode = "group"
)
# --- GRÁFICO 12: Heatmap de Precios ---
# Crear matriz de precios promedio
matriz_precios <- aggregate(Precio ~ Categoria + Segmento,
data = menu_data,
FUN = mean)
# Convertir a matriz
mat <- matrix(NA,
nrow = length(unique(matriz_precios$Categoria)),
ncol = length(unique(matriz_precios$Segmento)))
rownames(mat) <- unique(matriz_precios$Categoria)
colnames(mat) <- unique(matriz_precios$Segmento)
for(i in 1:nrow(matriz_precios)) {
mat[matriz_precios$Categoria[i], matriz_precios$Segmento[i]] <- matriz_precios$Precio[i]
}
# Crear heatmap
heatmap(mat,
col = colorRampPalette(c("white", "#FFD93D", "#FF6B6B"))(50),
main = "Mapa de Calor: Precio Promedio\nCategoría vs Segmento",
xlab = "Segmento de Precio",
ylab = "Categoría",
scale = "none",
margins = c(8, 12),
cexRow = 0.8,
cexCol = 0.8)
barplot(top10$Precio,
names.arg = top10$Nombre_Corto,
horiz = TRUE,
col = colorRampPalette(c("#FF6B6B", "#4ECDC4"))(10),
border = "white",
main = "Top 10: Productos Más Caros",
xlab = "Precio (Bs.)",
las = 1,
cex.names = 0.8)
text(top10$Precio - 5, 1:10, labels = paste("Bs.", top10$Precio),
col = "black", font = 2, cex = 0.9)
install.packages(c("rvest", "ggplot2", "dplyr"))
install.packages(c("rvest", "ggplot2", "dplyr"))
library(rvest)
library(ggplot2)
library(dplyr)
# URL de prueba para scraping
url <- "https://webscraper.io/test-sites/e-commerce/static/computers/laptops"
# Leer la página
webpage <- read_html(url)
# Extraer títulos de productos
titles <- webpage %>% html_nodes(".title") %>% html_attr("title")
# Extraer precios
prices <- webpage %>% html_nodes(".price") %>% html_text()
# Extraer títulos de productos
titles <- webpage %>% html_nodes(".title") %>% html_attr("title")
# Extraer precios
prices <- webpage %>% html_nodes(".price") %>% html_text()
# Limpiar precios: quitar símbolo $ y convertir a numérico
prices_num <- as.numeric(gsub("\\$", "", prices))
# Crear dataframe
productos <- data.frame(titulo = titles, precio = prices_num)
# Mostrar primeros productos
head(productos)
ggplot(productos, aes(x = precio)) +
geom_histogram(binwidth = 50, fill = "steelblue", color = "black") +
labs(title = "Distribución de precios de laptops",
x = "Precio (USD)", y = "Cantidad de productos") +
theme_minimal()
productos_top <- productos %>% arrange(desc(precio)) %>% head(5)
ggplot(productos_top, aes(x = reorder(titulo, precio), y = precio, fill = precio)) +
geom_col() +
coord_flip() +
labs(title = "Top 5 laptops más caras",
x = "Producto", y = "Precio (USD)") +
scale_fill_gradient(low = "lightgreen", high = "darkgreen") +
theme_minimal()
# esta funcion toma el num de pag y construye la url correcta
extraer_productos <- function(pagina_num) {
if (pagina_num == 1) {
url <- "https://webscraper.io/test-sites/e-commerce/static/computers/laptops"
} else {
url <- paste0("https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=", pagina_num)
}
webpage <- read_html(url)
titles <- webpage %>% html_nodes(".title") %>% html_attr("title")
prices <- webpage %>% html_nodes(".price") %>% html_text()
prices_num <- as.numeric(gsub("\\$", "", prices))
data.frame(titulo = titles, precio = prices_num, stringsAsFactors = FALSE)
}
# Número total de páginas (en este caso sabemos que son 10 o mas revisar)
total_paginas <- 10
# Extraer datos de todas las páginas y unir con lapply (itera sobre todas las pag)
lista_productos <- lapply(1:total_paginas, extraer_productos)
productos_todos <- bind_rows(lista_productos) #une todos los df en uno solo
# Mostrar resultados
print(productos_todos)
# Histograma de precios
ggplot(productos_todos, aes(x = precio)) +
geom_histogram(binwidth = 50, fill = "steelblue", color = "black") +
labs(title = "Distribución de precios de laptops (todas las páginas)",
x = "Precio (USD)", y = "Cantidad de productos") +
theme_minimal()
# Top 5 productos más caros
productos_top <- productos_todos %>% arrange(desc(precio)) %>% head(5)
ggplot(productos_top, aes(x = reorder(titulo, precio), y = precio, fill = precio)) +
geom_col() +
coord_flip() +
labs(title = "Top 5 laptops más caras",
x = "Producto", y = "Precio (USD)") +
scale_fill_gradient(low = "lightgreen", high = "darkgreen") +
theme_minimal()
pag1<- "https://www.octoparse.es/blog/los-10-sitios-web-m%C3%A1s-escrapeados"
pag2 <- "https://www.octoparse.es/blog/5-mejores-rastreadores-web-de-redes-sociales"
pag1 <- "https://www.octoparse.es/blog/los-10-sitios-web-m%C3%A1s-escrapeados"
pag3 <-"https://www.octoparse.es/blog/c%C3%B3mo-extraer-detalles-de-productos-de-mercado-libre"
pag1 <- "https://www.octoparse.es/blog/los-10-sitios-web-m%C3%A1s-escrapeados"
pag4 <- "https://www.octoparse.es/blog/c%C3%B3mo-raspar-anuncios-de-empleo-de-indeed"
# Definir enlaces: cada fila es un enlace de origen a destino
# se crea un df con enlaces entre nodos a paginas
enlaces <- data.frame(
origen = c(pag1, pag1, pag1),
destino = c(pag2, pag3, pag4)
)
# Se construye un grafo dirigido con igraph
grafo <- graph_from_data_frame(enlaces, directed = TRUE)
library(igraph)
library(ggraph)
library(tidygraph)
install.packages(c("igraph", "ggraph", "tidygraph"))
menu_top10 <- menu_completo %>%
arrange(desc(Precio)) %>%
slice_head(n = 10)
ggplot(menu_top10, aes(x = reorder(Nombre, Precio), y = Precio, fill = Precio)) +
geom_col(show.legend = FALSE) +
coord_flip() +
geom_text(aes(label = Precio), hjust = -0.1, size = 3) +
scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
labs(title = "Top 10 Menús más caros - Pollos Copacabana",
x = "Producto",
y = "Precio (Bs.)") +
theme_minimal(base_size = 12)
menu_bottom10 <- menu_completo %>%
arrange(Precio) %>%
slice_head(n = 10)
ggplot(menu_bottom10, aes(x = reorder(Nombre, Precio), y = Precio, fill = Precio)) +
geom_col(show.legend = FALSE) +
coord_flip() +
geom_text(aes(label = Precio), hjust = -0.1, size = 3) +
scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
labs(title = "Top 10 Menús más baratos - Pollos Copacabana",
x = "Producto",
y = "Precio (Bs.)") +
theme_minimal(base_size = 12)
